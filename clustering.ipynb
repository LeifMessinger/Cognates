{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_stuff import generate_all_clusters, score_cluster, create_groupings, evaluate_clusters\n",
    "\n",
    "def hierarchical_clustering(adj_matrix):\n",
    "    '''\n",
    "    Clusters an adjacency matrix using hierarchical clustering\n",
    "    '''\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    from scipy.cluster.hierarchy import linkage, fcluster\n",
    "    Z = linkage(adj_matrix, method='ward', metric='cosine', optimal_ordering=True)\n",
    "    predicted_clusters = fcluster(Z, t=.8, criterion='distance')\n",
    "\n",
    "    from collections import defaultdict\n",
    "    clusters_dict = defaultdict(list)\n",
    "    for index, cluster_id in enumerate(predicted_clusters):\n",
    "        clusters_dict[cluster_id].append(index)\n",
    "\n",
    "    predicted_clusters = list(clusters_dict.values())\n",
    "\n",
    "    return predicted_clusters, Z\n",
    "\n",
    "def cluster_grouping(word_array, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Cluster a group of words using the model to create adjacency matrix.\n",
    "    \n",
    "    Args:\n",
    "        word_array: List of tensors representing IPA words\n",
    "        model: Trained transformer model\n",
    "        device: Device to run model on\n",
    "    \n",
    "    Returns:\n",
    "        List of clusters, where each cluster is a list of indices\n",
    "    \"\"\"\n",
    "    n_words = len(word_array)\n",
    "    \n",
    "    # Create adjacency matrix\n",
    "    adj_matrix = torch.zeros((n_words, n_words))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_words):\n",
    "            for j in range(i + 1, n_words):\n",
    "                # Create word pair tensor\n",
    "                word_pair = torch.stack([word_array[i], word_array[j]], dim=0).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Create masks (True for padding tokens)\n",
    "                mask1 = (word_array[i] == 0).unsqueeze(0).to(device)\n",
    "                mask2 = (word_array[j] == 0).unsqueeze(0).to(device)\n",
    "                word_pair_masks = torch.stack([mask1, mask2], dim=1).to(device)\n",
    "                \n",
    "                # Get similarity score\n",
    "                similarity = model(word_pair, word_pair_masks).item()\n",
    "                \n",
    "                # Fill both triangles of the matrix\n",
    "                adj_matrix[i, j] = 1 - similarity\n",
    "                adj_matrix[j, i] = 1 - similarity\n",
    "\n",
    "    # Convert to squareform\n",
    "    from scipy.spatial.distance import squareform\n",
    "    adj_matrix = squareform(adj_matrix.cpu().numpy())\n",
    "\n",
    "    # Find best clustering\n",
    "    predicted_clusters = hierarchical_clustering(adj_matrix)\n",
    "    \n",
    "    return predicted_clusters\n",
    "\n",
    "def cluster_and_evaluate_all_meanings(df, model, ipa_to_ids, device='cpu'):\n",
    "    \"\"\"Process all word meanings and evaluate clustering using the modular approach.\"\"\"\n",
    "    # Step 1: Preprocessing\n",
    "    groupings = create_groupings(df, ipa_to_ids)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Step 2 & 3: Clustering and Evaluation\n",
    "    from tqdm.notebook import tqdm\n",
    "    for meaning, (word_array, cognate_class_label_array, phonological_words, meaning) in tqdm(groupings.items(), total=len(groupings), desc=\"Groupings\"):\n",
    "        #print(f\"\\nProcessing meaning: {meaning}\")\n",
    "        #print(f\"Number of words: {len(word_array)}\")\n",
    "        \n",
    "        # Step 2: Clustering\n",
    "        predicted_clusters, linkage = cluster_grouping(word_array, model, device)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        from scipy.cluster.hierarchy import dendrogram\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(linkage, labels=phonological_words, leaf_rotation=0, leaf_font_size=12, orientation='right')\n",
    "        plt.title('Meaning: ' + meaning)\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.show()\n",
    "        \n",
    "        # Step 3: Evaluation\n",
    "        evaluation_results = evaluate_clusters(predicted_clusters, cognate_class_label_array, meaning)\n",
    "        \n",
    "        results[meaning] = evaluation_results\n",
    "        \n",
    "        print(f\"Predicted clusters: {predicted_clusters}\")\n",
    "        print(f\"True classes: {cognate_class_label_array}\")\n",
    "        print(f\"Accuracy: {evaluation_results['accuracy']:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerCognateModel(\n",
       "  (embedder): Embedding(129, 34, padding_idx=0)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=34, out_features=34, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=34, bias=True)\n",
       "        (norm1): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=68, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (cosine_similarity): CosineSimilarity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/ielexData.csv')\n",
    "\n",
    "# Create IPA to ID mapping\n",
    "import joblib\n",
    "ipa_embedder = joblib.load(\"data/embeddings/34.joblib\")\n",
    "ipa_to_ids = ipa_embedder.char_to_idx\n",
    "\n",
    "# Example of using the modular approach:\n",
    "from transformer_stuff import TransformerCognateModel\n",
    "model = torch.load('TransformerCognateModel_34.pt', weights_only=False)\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e74bdb5b176460a9f52e583ce6ccb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating batches:   0%|          | 0/47274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "ielexPairsDf = pd.read_csv(\"data/ielexData.csv\")\n",
    "\n",
    "ielexPairsDf = ielexPairsDf[['Language', 'Meaning', 'Phonological Form', 'cc']].dropna()\n",
    "ielexPairsDf.columns = ['Language', 'meaning', 'word', 'cognate_class']\n",
    "\n",
    "ielexPairs = []\n",
    "\n",
    "for _, group in ielexPairsDf.groupby('meaning'):\n",
    "    entries = group.to_dict('records')\n",
    "    for w1, w2 in combinations(entries, 2):\n",
    "        word1 = str(w1['word'])\n",
    "        word2 = str(w2['word'])\n",
    "        label = int(w1['cognate_class'] == w2['cognate_class'])\n",
    "        ielexPairs.append((word1, word2, label))\n",
    "\n",
    "def preprocess(all_pairs, ipa_embedder, device):\n",
    "    from itertools import chain\n",
    "\n",
    "    max_length = 0\n",
    "    for pair in all_pairs:\n",
    "        for word in pair:\n",
    "            length = len(word)\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "    \n",
    "    batches = torch.empty((len(all_pairs), 2, max_length), dtype=torch.int, device=device)\n",
    "    batches_masks = torch.zeros((len(all_pairs), 2, max_length), dtype=torch.bool, device=device)\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "    for pair_index, pair in tqdm(enumerate(all_pairs), total=len(all_pairs), desc=\"Creating batches\"):\n",
    "        for word_index, word in enumerate(pair):\n",
    "            for letter_index, letter in enumerate(word):\n",
    "                batches[pair_index, word_index, letter_index] = ipa_embedder.char_to_idx[letter]\n",
    "            \n",
    "            batches_masks[pair_index, word_index, len(word):] = True\n",
    "\n",
    "    return (batches, batches_masks, max_length)\n",
    "\n",
    "all_pairs = [row[:2] for row in ielexPairs]\n",
    "all_labels = [row[2] for row in ielexPairs]\n",
    "\n",
    "ldistance_operations, ldistance_masks, max_length = preprocess(all_pairs, ipa_embedder, device=device)\n",
    "test_data = list(zip(ldistance_operations, ldistance_masks))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "test_loader = DataLoader(test_data, batch_size=1600, generator=torch.Generator(device=torch.get_default_device().type))\n",
    "\n",
    "# predicted_labels = []\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# for batch in tqdm(test_loader, desc=\"Testing IELEX against GLED\"):\n",
    "#     batch_predicted_labels = model(*batch).squeeze()\n",
    "#     batch_predicted_labels = (batch_predicted_labels > 0.5).int()\n",
    "#     predicted_labels.extend(batch_predicted_labels.tolist())\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(all_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Language</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Phonological Form</th>\n",
       "      <th>cc</th>\n",
       "      <th>ASJP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>Greek</td>\n",
       "      <td>few</td>\n",
       "      <td>ˈliʝi</td>\n",
       "      <td>few:I</td>\n",
       "      <td>liSi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>few</td>\n",
       "      <td>ˈmaɫku</td>\n",
       "      <td>few:H</td>\n",
       "      <td>maLku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>Russian</td>\n",
       "      <td>few</td>\n",
       "      <td>'maɫɔ</td>\n",
       "      <td>few:H</td>\n",
       "      <td>maLo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>Polish</td>\n",
       "      <td>few</td>\n",
       "      <td>ˈmawɔ</td>\n",
       "      <td>few:H</td>\n",
       "      <td>mawo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68</td>\n",
       "      <td>Ukrainian</td>\n",
       "      <td>few</td>\n",
       "      <td>ˈmaɫɔ</td>\n",
       "      <td>few:H</td>\n",
       "      <td>maLo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>124</td>\n",
       "      <td>French</td>\n",
       "      <td>head</td>\n",
       "      <td>tɛt</td>\n",
       "      <td>head:D</td>\n",
       "      <td>tEt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>135</td>\n",
       "      <td>Italian</td>\n",
       "      <td>head</td>\n",
       "      <td>'tɛsta</td>\n",
       "      <td>head:D</td>\n",
       "      <td>tEsta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>136</td>\n",
       "      <td>Romanian</td>\n",
       "      <td>head</td>\n",
       "      <td>kap</td>\n",
       "      <td>head:B</td>\n",
       "      <td>kap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>143</td>\n",
       "      <td>Breton</td>\n",
       "      <td>head</td>\n",
       "      <td>ˈpɛnː</td>\n",
       "      <td>head:E</td>\n",
       "      <td>pEn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>145</td>\n",
       "      <td>Irish</td>\n",
       "      <td>head</td>\n",
       "      <td>caːn̪ˠ, can̪ˠ</td>\n",
       "      <td>head:E</td>\n",
       "      <td>TanTan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4484 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0   Language Meaning Phonological Form      cc    ASJP\n",
       "0             11      Greek     few             ˈliʝi   few:I    liSi\n",
       "1             63  Bulgarian     few            ˈmaɫku   few:H   maLku\n",
       "2             65    Russian     few             'maɫɔ   few:H    maLo\n",
       "3             66     Polish     few             ˈmawɔ   few:H    mawo\n",
       "4             68  Ukrainian     few             ˈmaɫɔ   few:H    maLo\n",
       "...          ...        ...     ...               ...     ...     ...\n",
       "4479         124     French    head               tɛt  head:D     tEt\n",
       "4480         135    Italian    head            'tɛsta  head:D   tEsta\n",
       "4481         136   Romanian    head               kap  head:B     kap\n",
       "4482         143     Breton    head             ˈpɛnː  head:E     pEn\n",
       "4483         145      Irish    head     caːn̪ˠ, can̪ˠ  head:E  TanTan\n",
       "\n",
       "[4484 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4420b3abcd6f430a81c09ef3dcb73a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groupings:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted clusters: [[0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [1, 2, 21, 22, 23]]\n",
      "True classes: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'D', 'D', 'D']\n",
      "Accuracy: 0.841\n",
      "Predicted clusters: [[0, 6, 10, 11, 12, 13, 19], [1, 2, 3, 4, 5, 7, 8, 9, 14, 15, 16, 17, 18]]\n",
      "True classes: ['L', 'G', 'H', 'H', 'H', 'H', 'H', 'H', 'A', 'A', 'A', 'A', 'A', 'A', 'C', 'C', 'C', 'C', 'C', 'G']\n",
      "Accuracy: 0.547\n",
      "Predicted clusters: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17], [11, 12, 13, 18, 19, 20, 21]]\n",
      "True classes: ['J', 'A', 'P', 'H', 'H', 'H', 'H', 'W', 'I', 'I', 'I', 'B', 'B', 'B', 'F', 'F', 'F', 'F', 'F', 'G', 'G', 'G']\n",
      "Accuracy: 0.528\n",
      "Predicted clusters: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 18, 19], [10, 13, 14, 15, 16, 17]]\n",
      "True classes: ['G', 'D', 'G', 'G', 'H', 'T', 'H', 'A', 'A', 'A', 'F', 'A', 'A', 'F', 'F', 'F', 'F', 'F', 'M', 'F/Q']\n",
      "Accuracy: 0.595\n",
      "Predicted clusters: [[0, 1, 2, 3, 4, 5]]\n",
      "True classes: ['B', 'A', 'A', 'A', 'A', 'A']\n",
      "Accuracy: 0.667\n",
      "Predicted clusters: [[0, 1, 2, 4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [3, 5, 6, 7]]\n",
      "True classes: ['L', 'H', 'I', 'G', 'Y', 'G', 'G', 'G', 'A', 'H', 'A', 'A', 'A', 'A', 'A', 'E', 'E', 'E', 'E', 'E', 'F']\n",
      "Accuracy: 0.476\n",
      "Predicted clusters: [[0, 2, 3, 5, 7, 9, 10, 12, 14, 17, 19, 20, 21, 23, 29, 31], [1, 4, 6, 8, 11, 13, 15, 16, 18, 22, 24, 25, 26, 27, 28, 30, 32]]\n",
      "True classes: ['E', 'G', 'J', 'Q', 'J', 'Q', 'J', 'Q', 'J', 'Q', 'Q', 'J', 'B', 'N', 'Aj', 'U', 'Ai', 'U', 'Ai', 'U', 'R', 'V', 'I', 'T', 'R', 'S', 'J', 'R', 'R', 'R', 'R', 'J', 'R']\n",
      "Accuracy: 0.538\n",
      "Predicted clusters: [[0, 1, 2, 3, 4, 5, 7, 10, 11, 15, 16, 20], [6, 8, 9, 12, 13, 14, 17, 18, 19]]\n",
      "True classes: ['L', 'G', 'Ae', 'Q', 'P', 'Q', 'Ab', 'B', 'R', 'R', 'R', 'B', 'R', 'R', 'F', 'C', 'C', 'H', 'H', 'I', 'N']\n",
      "Accuracy: 0.543\n",
      "Predicted clusters: [[0, 1, 3, 9, 10, 12, 14, 16, 18, 19, 24, 27, 28], [2, 4, 5, 6, 11, 13, 17], [7, 8, 15, 20, 21, 22, 23, 25, 26]]\n",
      "True classes: ['Be', 'F', 'Az', 'G', 'Ay', 'Bb', 'K', 'R', 'R', 'Q', 'Aw', 'R', 'Q', 'M', 'Q', 'R', 'M', 'T', 'B', 'B', 'D', 'D', 'D', 'D', 'V', 'L', 'O', 'E', 'Aa']\n",
      "Accuracy: 0.690\n",
      "Predicted clusters: [[0, 2, 3, 4, 5, 6, 7, 14], [1, 12, 16, 17, 18, 19, 20, 21, 22], [8, 9, 10, 11, 13, 15]]\n",
      "True classes: ['K', 'L', 'E', 'B', 'B', 'B', 'B', 'B', 'G', 'G', 'G', 'G', 'B', 'H', 'J', 'G', 'B', 'B', 'B', 'B', 'B', 'B', 'F']\n",
      "Accuracy: 0.711\n",
      "Predicted clusters: [[0, 5, 7, 14, 16, 17, 18, 19, 20, 21], [1, 2, 3, 8, 9, 12, 13], [4, 6, 10, 11, 15, 24, 25], [22, 23]]\n",
      "True classes: ['N', 'At', 'D', 'A/Ab/F', 'A/Ac/F', 'V', 'A/F', 'A/H', 'V', 'L', 'Ao', 'W', 'A/H', 'A/H', 'K', 'A/U', 'M', 'F/H', 'F/H', 'F/H', 'F/H', 'F/H', 'Au', 'E', 'Aa/E', 'E/Z']\n",
      "Accuracy: 0.754\n",
      "Predicted clusters: [[0, 1, 3, 5, 17, 19, 20, 21, 22, 23], [2, 8, 11, 12], [4, 6, 7, 9, 10, 13, 14, 15, 16, 18]]\n",
      "True classes: ['I', 'Ak', 'Am', 'S', 'H', 'S', 'H', 'B', 'N', 'O', 'B', 'N', 'N', 'B', 'B', 'B', 'B', 'U', 'E', 'E', 'E', 'E', 'X', 'F']\n",
      "Accuracy: 0.725\n",
      "Predicted clusters: [[0, 3, 6, 7, 10, 18, 20, 22], [1, 2, 4, 12, 16, 21], [5, 8, 9, 11, 13, 14, 15, 17, 19]]\n",
      "True classes: ['B', 'C', 'C', 'E', 'Ai', 'Ah', 'F', 'F', 'O', 'O', 'A', 'O', 'G', 'A', 'A', 'D', 'D', 'D', 'D', 'D', 'N', 'Af/Ag', 'I']\n",
      "Accuracy: 0.688\n",
      "Predicted clusters: [[0, 1, 3, 5, 6, 7, 8, 16, 19], [2, 4, 14, 15, 17, 18, 20, 21], [9, 10, 11, 12, 13]]\n",
      "True classes: ['I', 'E', 'K', 'H', 'H', 'H', 'H', 'H', 'H', 'B', 'B', 'B', 'B', 'B', 'F', 'F', 'G', 'F', 'F', 'L', 'G', 'G']\n",
      "Accuracy: 0.766\n",
      "Predicted clusters: [[0, 2, 4, 5], [1, 3, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18]]\n",
      "True classes: ['A', 'I', 'J', 'S', 'J', 'J', 'B', 'B', 'B', 'B', 'B', 'B', 'K', 'G', 'G', 'G', 'G', 'K', 'Ad/H']\n",
      "Accuracy: 0.825\n",
      "Predicted clusters: [[0, 1, 5, 11, 12, 16, 19, 20], [2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 17, 18, 21, 22]]\n",
      "True classes: ['J', 'E', 'H', 'H', 'H', 'H', 'H', 'L', 'B', 'B', 'B', 'I', 'B', 'B', 'F', 'F', 'N', 'F', 'F', 'F', 'F', 'G', 'G']\n",
      "Accuracy: 0.490\n",
      "Predicted clusters: [[0, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 12, 15, 18, 19], [13, 14, 16, 17]]\n",
      "True classes: ['J', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'B', 'B', 'B', 'B', 'G', 'G', 'G', 'G', 'G', 'G', 'H', 'M']\n",
      "Accuracy: 0.795\n",
      "Predicted clusters: [[0, 2, 4, 5, 10, 12, 16, 17, 18], [1, 3], [6, 7, 8, 9, 11, 13, 14, 15]]\n",
      "True classes: ['C', 'L', 'F', 'F', 'F', 'C', 'A', 'A', 'A', 'A', 'H', 'A', 'H', 'A', 'A', 'A', 'A', 'A', 'D']\n",
      "Accuracy: 0.702\n",
      "Predicted clusters: [[0, 1, 2, 3, 4, 5, 6, 11, 14, 15, 16, 17, 18, 19, 20, 21], [7, 8, 9, 10, 12, 13]]\n",
      "True classes: ['H', 'A', 'F', 'F', 'F', 'F', 'F', 'B', 'B', 'B', 'B', 'B', 'L', 'B', 'G', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "Accuracy: 0.571\n",
      "Predicted clusters: [[0, 1, 2, 3, 5, 6, 11, 12, 14], [4, 7, 8, 9, 10, 13, 15, 16, 17]]\n",
      "True classes: ['I', 'Z', 'E', 'E', 'D', 'E', 'E', 'C', 'C', 'C', 'A', 'M', 'H', 'A', 'M', 'B', 'C', 'C']\n",
      "Accuracy: 0.647\n",
      "Predicted clusters: [[0, 2, 3, 4, 5, 6, 7, 8, 9, 13, 18], [1, 11, 12], [10, 14, 15, 16, 17]]\n",
      "True classes: ['P', 'E', 'E', 'E', 'E', 'E', 'O', 'T', 'O', 'O', 'N', 'A', 'A', 'O', 'L', 'L', 'L', 'L', 'O']\n",
      "Accuracy: 0.713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Process a single meaning group\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# meaning = 'few'\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# if meaning in groupings:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Or process all meanings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m all_results = \u001b[43mcluster_and_evaluate_all_meanings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mipa_to_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mcluster_and_evaluate_all_meanings\u001b[39m\u001b[34m(df, model, ipa_to_ids, device)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m meaning, (word_array, cognate_class_label_array, phonological_words, meaning) \u001b[38;5;129;01min\u001b[39;00m tqdm(groupings.items(), total=\u001b[38;5;28mlen\u001b[39m(groupings), desc=\u001b[33m\"\u001b[39m\u001b[33mGroupings\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m#print(f\"\\nProcessing meaning: {meaning}\")\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m#print(f\"Number of words: {len(word_array)}\")\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# Step 2: Clustering\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     predicted_clusters, linkage = \u001b[43mcluster_grouping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mcluster_grouping\u001b[39m\u001b[34m(word_array, model, device)\u001b[39m\n\u001b[32m     49\u001b[39m word_pair_masks = torch.stack([mask1, mask2], dim=\u001b[32m1\u001b[39m).to(device)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Get similarity score\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m similarity = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_pair_masks\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Fill both triangles of the matrix\u001b[39;00m\n\u001b[32m     55\u001b[39m adj_matrix[i, j] = \u001b[32m1\u001b[39m - similarity\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\transformer_stuff.py:99\u001b[39m, in \u001b[36mTransformerCognateModel.forward\u001b[39m\u001b[34m(self, word_pair, word_pair_masks)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, word_pair, word_pair_masks):\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \tenc1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_pair_masks\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, embedding_dim]\u001b[39;00m\n\u001b[32m    100\u001b[39m \tenc2 = \u001b[38;5;28mself\u001b[39m.encode_word(word_pair[:, \u001b[32m1\u001b[39m, :], word_pair_masks[:, \u001b[32m1\u001b[39m, :])  \u001b[38;5;66;03m# [batch_size, embedding_dim]\u001b[39;00m\n\u001b[32m    102\u001b[39m \t\u001b[38;5;66;03m# Compute cosine similarity\u001b[39;00m\n\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m \t\u001b[38;5;66;03m#similarity = torch.norm(enc1 - enc2, dim=1)\u001b[39;00m\n\u001b[32m    105\u001b[39m \t\u001b[38;5;66;03m#import random\u001b[39;00m\n\u001b[32m    106\u001b[39m \t\u001b[38;5;66;03m#combined = torch.cat([enc1, enc2] if random.choice([True, False]) else [enc2, enc1], dim=1)  # [batch_size, embedding_dim * 2]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\transformer_stuff.py:89\u001b[39m, in \u001b[36mTransformerCognateModel.encode_word\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     86\u001b[39m pos_encoded_x = \u001b[38;5;28mself\u001b[39m.pos_encoder(x)  \u001b[38;5;66;03m# [batch_size, seq_len, embedding_dim]\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m#print(f\"After pos encoding shape: {pos_encoded_x.shape}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m encoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_encoded_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len, embedding_dim]\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m#print(f\"After transformer shape: {encoded.shape}\")\u001b[39;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Option 1: Use mean pooling to get a fixed-size representation\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encoded.mean(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:514\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    511\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    522\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lsm0147\\Documents\\Cognates\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:881\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    877\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[32m    878\u001b[39m         merged_mask, mask_type = \u001b[38;5;28mself\u001b[39m.self_attn.merge_masks(\n\u001b[32m    879\u001b[39m             src_mask, src_key_padding_mask, src\n\u001b[32m    880\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_transformer_encoder_layer_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m            \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation_relu_or_gelu\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[38;5;66;03m# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\u001b[39;00m\n\u001b[32m    905\u001b[39m x = src\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Process a single meaning group\n",
    "# meaning = 'few'\n",
    "# if meaning in groupings:\n",
    "#     word_array, cognate_labels = groupings[meaning]\n",
    "#     clusters = cluster_grouping(word_array, model, device)\n",
    "#     results = evaluate_clusters(clusters, cognate_labels, meaning)\n",
    "#     print(f\"Results for '{meaning}': {results}\")\n",
    "\n",
    "# Or process all meanings\n",
    "all_results = cluster_and_evaluate_all_meanings(df, model, ipa_to_ids, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [r['accuracy'] for r in all_results.values()]\n",
    "print(f\"\\nOverall average accuracy: {np.mean(accuracies):.3f}\")\n",
    "\n",
    "f1s = [r['f1'] for r in all_results.values()]\n",
    "print(f\"\\nOverall f1 score: {np.mean(f1s):.3f}\")\n",
    "\n",
    "nmis = [r['nmi'] for r in all_results.values()]\n",
    "print(f\"\\nOverall nmi score: {np.mean(nmis):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from visualization_stuff import show_box_plot\n",
    "\n",
    "show_box_plot(accuracies, title=\"IELEX Clustering Accuracies\")\n",
    "show_box_plot(f1s, title=\"IELEX Clustering F1 Scores\")\n",
    "show_box_plot(nmis, title=\"IELEX Clustering NMI scores\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
